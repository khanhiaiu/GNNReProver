{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02af156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 13:52:32.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlightweight_graph.dataset\u001b[0m:\u001b[36mload_or_create\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mLoading lightweight graph dataset from lightweight_graph/data...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded to cuda:1. Training contexts: 249015\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from lightweight_graph.dataset import LightweightGraphDataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Generator, Tuple\n",
    "\n",
    "\n",
    "# Set PYTHONPATH to project root for the import\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) if \"scripts\" in os.getcwd() or \"notebooks\" in os.getcwd() else os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# --- Configuration ---\n",
    "SAVE_DIR = \"lightweight_graph/data\"\n",
    "# Set the primary device for data loading (e.g., 'cuda:1' or 'cuda:2')\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load and move data to the specified device ---\n",
    "dataset = LightweightGraphDataset.load_or_create(save_dir=SAVE_DIR)\n",
    "dataset.to(DEVICE)\n",
    "\n",
    "print(f\"Dataset loaded to {DEVICE}. Training contexts: {dataset.train_mask.sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d22700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_contexts(\n",
    "    dataset: LightweightGraphDataset,\n",
    "    split_indices: torch.Tensor,\n",
    "    batch_size: int\n",
    ") -> Generator[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Generator that yields mini-batches for GNN training or evaluation.\n",
    "    Each batch contains a subgraph with all premises and a slice of contexts.\n",
    "    \"\"\"\n",
    "    n_premises = dataset.premise_embeddings.size(0)\n",
    "    \n",
    "    # Pre-filter edges and labels for the entire split for efficiency\n",
    "    split_edge_mask = torch.isin(dataset.context_edge_index[1], split_indices)\n",
    "    split_context_edge_index = dataset.context_edge_index[:, split_edge_mask]\n",
    "    split_context_edge_attr = dataset.context_edge_attr[split_edge_mask]\n",
    "    \n",
    "    split_label_mask = torch.isin(dataset.context_premise_labels[0], split_indices)\n",
    "    split_context_premise_labels = dataset.context_premise_labels[:, split_label_mask]\n",
    "\n",
    "    for start in range(0, len(split_indices), batch_size):\n",
    "        end = min(start + batch_size, len(split_indices))\n",
    "        batch_global_indices = split_indices[start:end]\n",
    "        \n",
    "        batch_global_to_local_map = torch.full(\n",
    "            (batch_global_indices.max() + 1,), -1, dtype=torch.long, device=split_indices.device\n",
    "        )\n",
    "        batch_global_to_local_map[batch_global_indices] = torch.arange(\n",
    "            len(batch_global_indices), device=split_indices.device\n",
    "        )\n",
    "        \n",
    "        batch_context_embeddings = dataset.context_embeddings[batch_global_indices]\n",
    "        batch_context_file_indices = dataset.context_to_file_idx_map[batch_global_indices]\n",
    "        \n",
    "        batch_edge_mask = torch.isin(split_context_edge_index[1], batch_global_indices)\n",
    "        batch_context_edge_index_global = split_context_edge_index[:, batch_edge_mask]\n",
    "        batch_context_edge_attr = split_context_edge_attr[batch_edge_mask]\n",
    "        \n",
    "        batch_label_mask = torch.isin(split_context_premise_labels[0], batch_global_indices)\n",
    "        batch_labels_global = split_context_premise_labels[:, batch_label_mask]\n",
    "\n",
    "        # Use the new batch-specific map for shifting indices\n",
    "        batch_context_edge_index = batch_context_edge_index_global.clone()\n",
    "        batch_context_edge_index[1] = batch_global_to_local_map[batch_context_edge_index[1]] + n_premises\n",
    "        \n",
    "        batch_labels = batch_labels_global.clone()\n",
    "        batch_labels[0] = batch_global_to_local_map[batch_labels[0]]\n",
    "\n",
    "        all_batch_embeddings = torch.cat([dataset.premise_embeddings, batch_context_embeddings], dim=0)\n",
    "        all_batch_edge_index = torch.cat([dataset.premise_edge_index, batch_context_edge_index], dim=1)\n",
    "        all_batch_edge_attr = torch.cat([dataset.premise_edge_attr, batch_context_edge_attr], dim=0)\n",
    "\n",
    "        yield all_batch_embeddings, all_batch_edge_index, all_batch_edge_attr, batch_labels, batch_context_file_indices\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Abstract base class for a GNN-based retrieval model.\"\"\"\n",
    "    \n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(\"Subclasses must implement the training step.\")\n",
    "\n",
    "    def train_epoch(self, dataset: LightweightGraphDataset, batch_size: int) -> float:\n",
    "        train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "        train_generator = batchify_contexts(dataset, train_indices, batch_size)\n",
    "        total_loss, num_batches = 0.0, 0\n",
    "        pbar = tqdm(train_generator, desc=\"Training Epoch\")\n",
    "        for batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, _ in pbar:\n",
    "            loss = self.train_batch(batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels)\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "        return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int) -> torch.Tensor:\n",
    "        raise NotImplementedError(\"Subclasses must implement the prediction logic.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor, batch_context_file_indices: torch.Tensor, dataset: LightweightGraphDataset) -> Dict[str, float]:\n",
    "        n_premises = dataset.premise_embeddings.shape[0]\n",
    "        num_batch_contexts = batch_embeddings.shape[0] - n_premises\n",
    "        scores = self.get_predictions(batch_embeddings, batch_edge_index, batch_edge_attr, num_batch_contexts, n_premises)\n",
    "\n",
    "        # --- Create Accessibility Mask (This logic is now correct) ---\n",
    "        accessible_mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "        for i in range(num_batch_contexts):\n",
    "            context_file_idx = batch_context_file_indices[i].item()\n",
    "            \n",
    "            # 1. Premises in the same file are accessible\n",
    "            in_file_mask = (dataset.premise_to_file_idx_map == context_file_idx)\n",
    "            \n",
    "            # 2. Premises in imported files (transitive) are accessible\n",
    "            # This is a single lookup because file_dependency_edge_index IS the transitive closure.\n",
    "            dependency_file_indices = dataset.file_dependency_edge_index[1, dataset.file_dependency_edge_index[0] == context_file_idx]\n",
    "            imported_mask = torch.isin(dataset.premise_to_file_idx_map, dependency_file_indices)\n",
    "            \n",
    "            accessible_mask[i] = in_file_mask | imported_mask\n",
    "        \n",
    "        scores.masked_fill_(~accessible_mask, -torch.inf)\n",
    "        \n",
    "        # --- Metric Calculation ---\n",
    "        gt_mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "        gt_mask[batch_labels[0], batch_labels[1]] = True\n",
    "        num_positives = gt_mask.sum(dim=1)\n",
    "        valid_contexts = num_positives > 0\n",
    "        if not valid_contexts.any(): return {'R@1': 0.0, 'R@10': 0.0, 'MRR': 0.0}\n",
    "\n",
    "        top_10_indices = scores.topk(k=10, dim=1).indices\n",
    "        top_10_hits = gt_mask.gather(1, top_10_indices)\n",
    "\n",
    "        recall_at_1 = (top_10_hits[:, 0][valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "        recall_at_10 = (top_10_hits.sum(dim=1)[valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "        \n",
    "        sorted_indices = scores.argsort(dim=1, descending=True)\n",
    "        sorted_gt = gt_mask.gather(1, sorted_indices)\n",
    "        first_hit_rank = torch.argmax(sorted_gt[valid_contexts].int(), dim=1) + 1\n",
    "        mrr = (1.0 / first_hit_rank).mean().item()\n",
    "        \n",
    "        return {'R@1': recall_at_1, 'R@10': recall_at_10, 'MRR': mrr}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self, dataset: LightweightGraphDataset, split: str, batch_size: int) -> Dict[str, float]:\n",
    "        mask = getattr(dataset, f\"{split}_mask\", None)\n",
    "        if mask is None: raise ValueError(f\"Invalid split: {split}\")\n",
    "        \n",
    "        split_indices = mask.nonzero(as_tuple=True)[0]\n",
    "        eval_generator = batchify_contexts(dataset, split_indices, batch_size)\n",
    "        \n",
    "        all_metrics = []\n",
    "        pbar = tqdm(eval_generator, desc=f\"Evaluating on {split} split\")\n",
    "        for batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, batch_context_file_indices in pbar:\n",
    "            metrics = self.eval_batch(batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, batch_context_file_indices, dataset)\n",
    "            all_metrics.append(metrics)\n",
    "            pbar.set_postfix(metrics)\n",
    "\n",
    "        if not all_metrics: return {'R@1': 0.0, 'R@10': 0.0, 'MRR': 0.0}\n",
    "        \n",
    "        final_metrics = {key: torch.tensor([m[key] for m in all_metrics]).mean().item() for key in all_metrics[0]}\n",
    "        \n",
    "        print(f\"\\n--- Evaluation Results for '{split}' split ---\")\n",
    "        print(f\"  Recall@1:  {final_metrics['R@1']:.4f}\")\n",
    "        print(f\"  Recall@10: {final_metrics['R@10']:.4f}\")\n",
    "        print(f\"  MRR:       {final_metrics['MRR']:.4f}\")\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "        return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Initialized BaselineModel (no GNN, no training).\")\n",
    "\n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor) -> float:\n",
    "        return 0.0\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int) -> torch.Tensor:\n",
    "        # The input batch_embeddings are the initial LM embeddings.\n",
    "        # We simply ignore the edge_index and edge_attr.\n",
    "        initial_premise_embs = batch_embeddings[:n_premises]\n",
    "        initial_context_embs = batch_embeddings[n_premises:]\n",
    "\n",
    "        # L2-normalize for cosine similarity calculation.\n",
    "        premise_embs_norm = F.normalize(initial_premise_embs, p=2, dim=1)\n",
    "        context_embs_norm = F.normalize(initial_context_embs, p=2, dim=1)\n",
    "\n",
    "        # Compute similarity scores via matrix multiplication.\n",
    "        scores = torch.mm(context_embs_norm, premise_embs_norm.T)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7914568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBaselineModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Initialized RandomBaselineModel (random scores, no training).\")\n",
    "\n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor) -> float:\n",
    "        return 0.0\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int) -> torch.Tensor:\n",
    "        # Generate random scores for each context-premise pair.\n",
    "        scores = torch.rand((num_batch_contexts, n_premises), device=batch_embeddings.device)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4d12073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline_model = BaselineModel()\n",
    "#baseline_val_metrics = baseline_model.eval(dataset, split=\"train\", batch_size=5120)\n",
    "#baseline_val_metrics = baseline_model.eval(dataset, split=\"val\", batch_size=256)\n",
    "#baseline_test_metrics = baseline_model.eval(dataset, split=\"test\", batch_size=256)\n",
    "\n",
    "#random_model = RandomBaselineModel()\n",
    "#random_val_metrics = random_model.eval(dataset, split=\"train\", batch_size=5120)\n",
    "#random_val_metrics = random_model.eval(dataset, split=\"val\", batch_size=256)\n",
    "#random_test_metrics = random_model.eval(dataset, split=\"test\", batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2550ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from numpy import negative\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.utils import dropout_edge\n",
    "from typing import Tuple, Literal\n",
    "\n",
    "from IPython import embed\n",
    "\n",
    "# from your_abstract_model_file import Model\n",
    "# from lightweight_graph.dataset import LightweightGraphDataset\n",
    "\n",
    "def calculate_metrics(scores: torch.Tensor, gt_mask: torch.Tensor) -> float:\n",
    "    num_positives = gt_mask.sum(dim=1)\n",
    "    valid_contexts = num_positives > 0\n",
    "    if not valid_contexts.any():\n",
    "        return {\"R@1\": 0.0, \"R@10\": 0.0, \"MRR\": 0.0}\n",
    "\n",
    "    # calculate R@1, R@10, MRR\n",
    "    top_10_indices = scores.topk(k=10, dim=1).indices\n",
    "    top_10_hits = gt_mask.gather(1, top_10_indices)\n",
    "\n",
    "    tr1 = (torch.ones_like(top_10_hits)[:, 0][valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "    tr10 = (torch.ones_like(top_10_hits).sum(dim=1)[valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "\n",
    "    recall_at_1 = (top_10_hits[:, 0][valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "    recall_at_10 = (top_10_hits.sum(dim=1)[valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "\n",
    "    # compute reciprocal rank\n",
    "    #ranks = torch.arange(1, 11, device=scores.device).float()  # [1,2,...,10]\n",
    "    #reciprocal_ranks = (top_10_hits * (1.0 / ranks)).max(dim=1).values\n",
    "    #mrr = reciprocal_ranks[valid_contexts].mean().item()\n",
    "\n",
    "    return {\"R@1\": recall_at_1, \"R@10\": recall_at_10, \"R@1 upper bound\" : tr1, \"R@10 upper bound\" : tr10}#, \"MRR\": mrr}\n",
    "\n",
    "class HeadAttentionScoring(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, aggregation: Literal[\"logsumexp\", \"mean\", \"max\", \"gated\"]):\n",
    "        super(HeadAttentionScoring, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.score_W = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "        if aggregation == \"gated\":\n",
    "            self.gate_W = nn.Linear(embedding_dim, num_heads)\n",
    "\n",
    "    def forward(self, premise_embs: torch.Tensor, context_embs: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = context_embs.size(0)\n",
    "        n_premises = premise_embs.size(0)\n",
    "\n",
    "        context_embs = self.score_W(context_embs)  # (batch_size, embedding_dim)\n",
    "        # reshape for multi-head\n",
    "        context_embs = context_embs.view(batch_size, self.num_heads, self.embedding_dim // self.num_heads)  # (batch_size, num_heads, head_dim)\n",
    "        premise_embs = premise_embs.view(n_premises, self.num_heads, self.embedding_dim // self.num_heads)  # (n_premises, num_heads, head_dim)\n",
    "        # compute attention scores to get (batch_size, n_premises, num_heads)\n",
    "        scores = torch.einsum('bhd, phd -> bhp', context_embs, premise_embs)  # (batch_size, num_heads, n_premises)\n",
    "        scores = scores.permute(0, 2, 1)  # (batch_size, n_premises, num_heads)\n",
    "\n",
    "        if self.aggregation == \"max\":\n",
    "            scores, _ = scores.max(dim=-1)  # (batch_size, n_premises)\n",
    "        elif self.aggregation == \"mean\":\n",
    "            scores = scores.mean(dim=-1)  # (batch_size, n_premises)\n",
    "        elif self.aggregation == \"logsumexp\":\n",
    "            scores = torch.logsumexp(scores, dim=-1)  # (batch_size, n_premises)\n",
    "        elif self.aggregation == \"gated\":\n",
    "            score_gates = self.gate_W(context_embs.mean(dim=1))  # (batch_size, num_heads)\n",
    "            # apply softmax to get weights\n",
    "            score_gates = F.softmax(score_gates, dim=-1)  # (batch_size, num_heads)\n",
    "            scores = (scores * score_gates.unsqueeze(1)).sum(dim=-1)  # (batch_size, n_premises)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation method: {self.aggregation}\")\n",
    "\n",
    "        return scores  # (batch_size, n_premises)\n",
    "\n",
    "class TestModel(Model, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: LightweightGraphDataset,\n",
    "        hidden_dim: int,\n",
    "        aggregation: Literal[\"mean\", \"max\", \"logsumexp\", \"gated\"],\n",
    "        n_heads: int,\n",
    "        lr: float = 1e-4,\n",
    "        loss : Literal[\"bce\", \"mse\"] = \"mse\",\n",
    "    ):\n",
    "        Model.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.embedding_dim = dataset.premise_embeddings.shape[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_relations = len(dataset.edge_types_map)\n",
    "        \n",
    "        self.random_premise_embeds = nn.Embedding(dataset.premise_embeddings.shape[0], self.hidden_dim)\n",
    "        self.random_premise_embed_for_context = nn.Embedding(dataset.premise_embeddings.shape[0], self.embedding_dim)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        # Keep track of metrics during hard mining\n",
    "        self.last_hard_mining_recall = 0.0\n",
    "        self.loss = loss\n",
    "\n",
    "        self.rgcn = RGCNConv(\n",
    "            in_channels=self.embedding_dim,\n",
    "            out_channels=self.hidden_dim,\n",
    "            num_relations=2,\n",
    "        )\n",
    "\n",
    "        self.scoring = HeadAttentionScoring(embedding_dim=self.hidden_dim, num_heads=n_heads, aggregation=aggregation)\n",
    "        \n",
    "        print(f\"Initialized RGCNModel with {self.num_relations} relations, hidden_dim={self.hidden_dim}\")\n",
    "\n",
    "    def forward(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, n_premises: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        expected_dtype = torch.float32\n",
    "        initial_premise_embs = self.random_premise_embeds.weight.to(expected_dtype)\n",
    "        \n",
    "        initial_context_embs = torch.zeros_like(batch_embeddings[n_premises:]).to(expected_dtype)\n",
    "        initial_premise_emb_for_context = self.random_premise_embed_for_context.weight.to(expected_dtype)\n",
    "        batch_embeddings_for_context = torch.cat([initial_premise_emb_for_context, initial_context_embs], dim=0)\n",
    "\n",
    "\n",
    "        refined_context_embs = self.rgcn(batch_embeddings_for_context, batch_edge_index, batch_edge_attr)\n",
    "        \n",
    "        # refine the premise embeddings first\n",
    "        return initial_premise_embs, refined_context_embs[n_premises:]\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int, squash01 : bool = True) -> torch.Tensor:\n",
    "        final_premise_embs, final_context_embs = self.forward(batch_embeddings, batch_edge_index, batch_edge_attr, n_premises)\n",
    "        return self.scoring(final_premise_embs, final_context_embs)\n",
    "\n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor, i) -> torch.Tensor:\n",
    "        self.train()\n",
    "        n_premises = self.premise_embeddings_shape[0]\n",
    "        num_batch_contexts = batch_embeddings.shape[0] - n_premises\n",
    "        logits_tensor = self.get_predictions(batch_embeddings, batch_edge_index, batch_edge_attr, num_batch_contexts, n_premises, squash01 = False)\n",
    "        \n",
    "        targets_tensor = torch.zeros_like(logits_tensor)\n",
    "        pos_context_indices = batch_labels[0]\n",
    "        pos_premise_indices = batch_labels[1]\n",
    "        targets_tensor[pos_context_indices, pos_premise_indices] = 1.0\n",
    "\n",
    "        # report R@10 during training for monitoring\n",
    "        self.last_metrics = calculate_metrics(logits_tensor, targets_tensor)\n",
    "        \n",
    "        # --- Weighted Loss Calculation (works for all strategies) ---\n",
    "        n_negative = (targets_tensor == 0).sum().item()\n",
    "        n_positive = (targets_tensor == 1).sum().item()\n",
    "\n",
    "        if n_positive == 0 or n_negative == 0:\n",
    "            # Handle edge case where we have only one class\n",
    "            weights = torch.ones_like(logits_tensor)\n",
    "        else:\n",
    "            # Calculate class weights to balance the loss\n",
    "            pos_weight = n_negative / n_positive  # Higher weight for minority class\n",
    "            weights = torch.ones_like(logits_tensor)\n",
    "            weights[targets_tensor == 1] = pos_weight\n",
    "\n",
    "        probs = torch.sigmoid(logits_tensor)\n",
    "        unweighted_loss = F.mse_loss(probs, targets_tensor, reduction='none')\n",
    "        weighted_loss = (unweighted_loss * weights).mean()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            embed()\n",
    "        return weighted_loss\n",
    "\n",
    "    def train_epoch(self, dataset: LightweightGraphDataset, batch_size: int, accumulation_steps: int = 1) -> float:\n",
    "        self.premise_embeddings_shape = dataset.premise_embeddings.shape\n",
    "        train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "        train_generator = batchify_contexts(dataset, train_indices, batch_size)\n",
    "        \n",
    "        total_loss, num_batches_processed = 0.0, 0\n",
    "        self.optimizer.zero_grad()\n",
    "        pbar = tqdm(enumerate(train_generator), total=len(train_indices)//batch_size, desc=\"Training Epoch\")\n",
    "        for i, (batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, _) in pbar:\n",
    "            if (i !=0):\n",
    "                assert 0\n",
    "            for j in range(10000):\n",
    "                loss = self.train_batch(batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, j)\n",
    "                loss = loss / accumulation_steps\n",
    "                memory = torch.cuda.memory_allocated(DEVICE)/1e9 if torch.cuda.is_available() else 0.0\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "                log_dict = {\"loss\": f\"{loss.item() * accumulation_steps:.4f}\", \"memory (GB)\": f\"{memory}\"}\n",
    "                log_dict.update(self.last_metrics)\n",
    "                pbar.set_postfix(log_dict)\n",
    "\n",
    "                total_loss += loss.item() * accumulation_steps\n",
    "                num_batches_processed += 1\n",
    "                \n",
    "        if num_batches_processed % accumulation_steps != 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        return total_loss / num_batches_processed if num_batches_processed > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35a662e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RGCNModel with 2 relations, hidden_dim=512\n",
      "\n",
      "--- [\"All\" Negatives] Epoch 1/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/243 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":983, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- [\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mAll\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m Negatives] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     avg_loss = testmodel.train_epoch(dataset, batch_size=BATCH_SIZE, accumulation_steps=ACCUMULATION_STEPS)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnd of Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Average Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Evaluate on validation set after each epoch\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mTestModel.train_epoch\u001b[39m\u001b[34m(self, dataset, batch_size, accumulation_steps)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.train_batch(batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, j)\n\u001b[32m    179\u001b[39m     loss = loss / accumulation_steps\n\u001b[32m    180\u001b[39m     memory = torch.cuda.memory_allocated(DEVICE)/\u001b[32m1e9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mTestModel.train_batch\u001b[39m\u001b[34m(self, batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, i)\u001b[39m\n\u001b[32m    133\u001b[39m n_premises = \u001b[38;5;28mself\u001b[39m.premise_embeddings_shape[\u001b[32m0\u001b[39m]\n\u001b[32m    134\u001b[39m num_batch_contexts = batch_embeddings.shape[\u001b[32m0\u001b[39m] - n_premises\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m logits_tensor = \u001b[38;5;28mself\u001b[39m.get_predictions(batch_embeddings, batch_edge_index, batch_edge_attr, num_batch_contexts, n_premises, squash01 = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    137\u001b[39m targets_tensor = torch.zeros_like(logits_tensor)\n\u001b[32m    138\u001b[39m pos_context_indices = batch_labels[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mTestModel.get_predictions\u001b[39m\u001b[34m(self, batch_embeddings, batch_edge_index, batch_edge_attr, num_batch_contexts, n_premises, squash01)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: \u001b[38;5;28mint\u001b[39m, n_premises: \u001b[38;5;28mint\u001b[39m, squash01 : \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     final_premise_embs, final_context_embs = \u001b[38;5;28mself\u001b[39m.forward(batch_embeddings, batch_edge_index, batch_edge_attr, n_premises)\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scoring(final_premise_embs, final_context_embs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mTestModel.forward\u001b[39m\u001b[34m(self, batch_embeddings, batch_edge_index, batch_edge_attr, n_premises)\u001b[39m\n\u001b[32m    118\u001b[39m initial_premise_emb_for_context = \u001b[38;5;28mself\u001b[39m.random_premise_embed_for_context.weight.to(expected_dtype)\n\u001b[32m    119\u001b[39m batch_embeddings_for_context = torch.cat([initial_premise_emb_for_context, initial_context_embs], dim=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m refined_context_embs = \u001b[38;5;28mself\u001b[39m.rgcn(batch_embeddings_for_context, batch_edge_index, batch_edge_attr)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# refine the premise embeddings first\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m initial_premise_embs, refined_context_embs[n_premises:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/nn/conv/rgcn_conv.py:267\u001b[39m, in \u001b[36mRGCNConv.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_type)\u001b[39m\n\u001b[32m    260\u001b[39m                 out = out + \u001b[38;5;28mself\u001b[39m.propagate(\n\u001b[32m    261\u001b[39m                     tmp,\n\u001b[32m    262\u001b[39m                     x=weight[i, x_l],\n\u001b[32m    263\u001b[39m                     edge_type_ptr=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    264\u001b[39m                     size=size,\n\u001b[32m    265\u001b[39m                 )\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m                 h = \u001b[38;5;28mself\u001b[39m.propagate(tmp, x=x_l, edge_type_ptr=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    268\u001b[39m                                    size=size)\n\u001b[32m    269\u001b[39m                 out = out + (h @ weight[i])\n\u001b[32m    271\u001b[39m root = \u001b[38;5;28mself\u001b[39m.root\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torch_geometric.nn.conv.rgcn_conv_RGCNConv_propagate_l7dojjnq.py:241\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, edge_type_ptr, size)\u001b[39m\n\u001b[32m    232\u001b[39m             kwargs = CollectArgs(\n\u001b[32m    233\u001b[39m                 x_j=kwargs.x_j,\n\u001b[32m    234\u001b[39m                 edge_type_ptr=kwargs.edge_type_ptr,\n\u001b[32m   (...)\u001b[39m\u001b[32m    237\u001b[39m                 dim_size=hook_kwargs[\u001b[33m'\u001b[39m\u001b[33mdim_size\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    238\u001b[39m             )\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m out = \u001b[38;5;28mself\u001b[39m.aggregate(\n\u001b[32m    242\u001b[39m     out,\n\u001b[32m    243\u001b[39m     index=kwargs.index,\n\u001b[32m    244\u001b[39m     ptr=kwargs.ptr,\n\u001b[32m    245\u001b[39m     dim_size=kwargs.dim_size,\n\u001b[32m    246\u001b[39m )\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:594\u001b[39m, in \u001b[36mMessagePassing.aggregate\u001b[39m\u001b[34m(self, inputs, index, ptr, dim_size)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maggregate\u001b[39m(\n\u001b[32m    578\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    579\u001b[39m     inputs: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    583\u001b[39m ) -> Tensor:\n\u001b[32m    584\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[33;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[32m    586\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    592\u001b[39m \u001b[33;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[32m    593\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,\n\u001b[32m    595\u001b[39m                             dim=\u001b[38;5;28mself\u001b[39m.node_dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/experimental.py:117\u001b[39m, in \u001b[36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[33m'\u001b[39m\u001b[33mdisable_dynamic_shapes\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[32m    120\u001b[39m         index = required_args_pos[required_arg]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/nn/aggr/base.py:139\u001b[39m, in \u001b[36mAggregation.__call__\u001b[39m\u001b[34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index.numel() > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dim_size <= \u001b[38;5;28mint\u001b[39m(index.max()):\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncountered invalid \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdim_size\u001b[39m\u001b[33m'\u001b[39m\u001b[33m (got \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m                          \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m but expected \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m                          \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>= \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index.max())\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/nn/aggr/base.py:131\u001b[39m, in \u001b[36mAggregation.__call__\u001b[39m\u001b[34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     dim_size = \u001b[38;5;28mint\u001b[39m(index.max()) + \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index.numel() > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(x, index=index, ptr=ptr, dim_size=dim_size,\n\u001b[32m    132\u001b[39m                             dim=dim, **kwargs)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/nn/aggr/basic.py:36\u001b[39m, in \u001b[36mMeanAggregation.forward\u001b[39m\u001b[34m(self, x, index, ptr, dim_size, dim)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     34\u001b[39m             ptr: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     35\u001b[39m             dim: \u001b[38;5;28mint\u001b[39m = -\u001b[32m2\u001b[39m) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reduce(x, index, ptr, dim_size, dim, reduce=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/nn/aggr/base.py:185\u001b[39m, in \u001b[36mAggregation.reduce\u001b[39m\u001b[34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAggregation requires \u001b[39m\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to be specified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scatter(x, index, dim, dim_size, reduce)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch_geometric/utils/_scatter.py:85\u001b[39m, in \u001b[36mscatter\u001b[39m\u001b[34m(src, index, dim, dim_size, reduce)\u001b[39m\n\u001b[32m     82\u001b[39m     index = broadcast(index, src, dim)\n\u001b[32m     83\u001b[39m     out = src.new_zeros(size).scatter_add_(dim, index, src)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out / broadcast(count, out, dim)\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# For \"min\" and \"max\" reduction, we prefer `scatter_reduce_` on CPU or\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# in case the input does not require gradients:\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mamin\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mamax\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[31mRuntimeError\u001b[39m: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":983, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-2\n",
    "# Note: For \"all\" strategy, a smaller batch size is needed due to the large logit matrix\n",
    "BATCH_SIZE = 1024\n",
    "ACCUMULATION_STEPS = 1 # Effective batch size = 256 * 16 = 4096\n",
    "EPOCHS = 100\n",
    "POSITIVE_LOSS_WEIGHT = 100.0 # Must be high for the \"all\" strategy\n",
    "\n",
    "# --- Instantiate the Model ---\n",
    "testmodel = TestModel(\n",
    "    dataset=dataset, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    lr=LEARNING_RATE,\n",
    "    aggregation=\"logsumexp\",\n",
    "    n_heads=8,\n",
    ")\n",
    "testmodel.to(DEVICE)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- [\\\"All\\\" Negatives] Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    avg_loss = testmodel.train_epoch(dataset, batch_size=BATCH_SIZE, accumulation_steps=ACCUMULATION_STEPS)\n",
    "    print(f\"End of Epoch {epoch+1}, Average Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set after each epoch\n",
    "    testmodel.eval(dataset, split=\"val\", batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Batch Label Duplication ---\n",
      "Total positive labels in batch: 1745\n",
      "Unique premises to be retrieved: 913\n",
      "\n",
      "--- Duplication Stats ---\n",
      "Number of unique premises that are duplicated: 353 (out of 913)\n",
      "Percentage of unique premises that are duplicated: 38.66%\n",
      "Total labels pointing to duplicated premises: 1185 (out of 1745)\n",
      "Percentage of labels that are for duplicated premises: 67.91%\n",
      "\n",
      "This means 832 times, the model must reuse a single premise embedding for different contexts.\n",
      "On average, a duplicated premise is required 3.36 times within this batch.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# %% Cell to Analyze Label Duplication in a Batch\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_batch_label_duplication(\n",
    "    dataset: LightweightGraphDataset,\n",
    "    batch_global_indices: torch.Tensor\n",
    "):\n",
    "    print(\"--- Analyzing Batch Label Duplication ---\")\n",
    "\n",
    "    split_label_mask = torch.isin(dataset.context_premise_labels[0], batch_global_indices)\n",
    "    batch_labels_global = dataset.context_premise_labels[:, split_label_mask]\n",
    "\n",
    "    if batch_labels_global.shape[1] == 0:\n",
    "        print(\"No positive labels found in this batch.\")\n",
    "        return\n",
    "\n",
    "    positive_premise_indices = batch_labels_global[1]\n",
    "    unique_premises, counts = torch.unique(positive_premise_indices, return_counts=True)\n",
    "    \n",
    "    total_labels = len(positive_premise_indices)\n",
    "    num_unique_premises = len(unique_premises)\n",
    "    \n",
    "    print(f\"Total positive labels in batch: {total_labels}\")\n",
    "    print(f\"Unique premises to be retrieved: {num_unique_premises}\")\n",
    "\n",
    "    duplicated_mask = counts > 1\n",
    "    num_duplicated_premises = duplicated_mask.sum().item()\n",
    "    \n",
    "    if num_duplicated_premises == 0:\n",
    "        print(\"No premise is a correct label for more than one context in this batch.\")\n",
    "        return\n",
    "\n",
    "    duplicated_premise_ids = unique_premises[duplicated_mask]\n",
    "    duplicated_premise_counts = counts[duplicated_mask]\n",
    "    \n",
    "    total_labels_involved_in_duplication = duplicated_premise_counts.sum().item()\n",
    "\n",
    "    num_redundant_labels = (duplicated_premise_counts - 1).sum().item()\n",
    "\n",
    "    avg_duplication_factor = duplicated_premise_counts.float().mean().item()\n",
    "    \n",
    "    print(f\"\\n--- Duplication Stats ---\")\n",
    "    print(f\"Number of unique premises that are duplicated: {num_duplicated_premises} (out of {num_unique_premises})\")\n",
    "    print(f\"Percentage of unique premises that are duplicated: {num_duplicated_premises / num_unique_premises:.2%}\")\n",
    "    print(f\"Total labels pointing to duplicated premises: {total_labels_involved_in_duplication} (out of {total_labels})\")\n",
    "    print(f\"Percentage of labels that are for duplicated premises: {total_labels_involved_in_duplication / total_labels:.2%}\")\n",
    "    print(f\"\\nThis means {num_redundant_labels} times, the model must reuse a single premise embedding for different contexts.\")\n",
    "    print(f\"On average, a duplicated premise is required {avg_duplication_factor:.2f} times within this batch.\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "# --- How to use it ---\n",
    "# Get the first batch of training indices to test\n",
    "train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "first_batch_indices = train_indices[:BATCH_SIZE]\n",
    "\n",
    "# Run the analysis\n",
    "analyze_batch_label_duplication(dataset, first_batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell to Analyze Label Duplication in a Batch\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_batch_label_duplication(\n",
    "    dataset: LightweightGraphDataset,\n",
    "    batch_global_indices: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes the duplication of ground-truth premises within a single batch.\n",
    "\n",
    "    This helps quantify the \"representation bottleneck\" problem, where the model\n",
    "    must learn a single embedding for a premise that needs to be retrieved by\n",
    "    multiple, different contexts in the same batch.\n",
    "    \"\"\"\n",
    "    print(\"--- Analyzing Batch Label Duplication ---\")\n",
    "\n",
    "    # 1. Find all labels that are relevant to this specific batch of contexts.\n",
    "    # This logic is borrowed from the batchify_contexts generator.\n",
    "    split_label_mask = torch.isin(dataset.context_premise_labels[0], batch_global_indices)\n",
    "    batch_labels_global = dataset.context_premise_labels[:, split_label_mask]\n",
    "\n",
    "    if batch_labels_global.shape[1] == 0:\n",
    "        print(\"No positive labels found in this batch.\")\n",
    "        return\n",
    "\n",
    "    # 2. Isolate the premise indices from the labels. These are the items being retrieved.\n",
    "    positive_premise_indices = batch_labels_global[1]\n",
    "\n",
    "    # 3. Count the occurrences of each unique premise index.\n",
    "    unique_premises, counts = torch.unique(positive_premise_indices, return_counts=True)\n",
    "    \n",
    "    total_labels = len(positive_premise_indices)\n",
    "    num_unique_premises = len(unique_premises)\n",
    "    \n",
    "    print(f\"Total positive labels in batch: {total_labels}\")\n",
    "    print(f\"Unique premises to be retrieved: {num_unique_premises}\")\n",
    "\n",
    "    # 4. Identify the duplicated premises and quantify the duplication.\n",
    "    duplicated_mask = counts > 1\n",
    "    num_duplicated_premises = duplicated_mask.sum().item()\n",
    "    \n",
    "    if num_duplicated_premises == 0:\n",
    "        print(\"No premise is a correct label for more than one context in this batch.\")\n",
    "        return\n",
    "\n",
    "    # 5. Calculate statistics on the duplicates.\n",
    "    duplicated_premise_ids = unique_premises[duplicated_mask]\n",
    "    duplicated_premise_counts = counts[duplicated_mask]\n",
    "    \n",
    "    # This is the total number of labels that point to a premise that is needed more than once.\n",
    "    # For example, if premise P is needed 3 times, it contributes 3 to this sum.\n",
    "    total_labels_involved_in_duplication = duplicated_premise_counts.sum().item()\n",
    "\n",
    "    # This is the number of \"extra\" pulls on the same embedding.\n",
    "    # If premise P is needed 3 times, it has 2 \"extra\" pulls.\n",
    "    num_redundant_labels = (duplicated_premise_counts - 1).sum().item()\n",
    "\n",
    "    avg_duplication_factor = duplicated_premise_counts.float().mean().item()\n",
    "    \n",
    "    print(f\"\\n--- Duplication Stats ---\")\n",
    "    print(f\"Number of unique premises that are duplicated: {num_duplicated_premises} (out of {num_unique_premises})\")\n",
    "    print(f\"Percentage of unique premises that are duplicated: {num_duplicated_premises / num_unique_premises:.2%}\")\n",
    "    print(f\"Total labels pointing to duplicated premises: {total_labels_involved_in_duplication} (out of {total_labels})\")\n",
    "    print(f\"Percentage of labels that are for duplicated premises: {total_labels_involved_in_duplication / total_labels:.2%}\")\n",
    "    print(f\"\\nThis means {num_redundant_labels} times, the model must reuse a single premise embedding for different contexts.\")\n",
    "    print(f\"On average, a duplicated premise is required {avg_duplication_factor:.2f} times within this batch.\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "# --- How to use it ---\n",
    "# Get the first batch of training indices to test\n",
    "BATCH_SIZE = 1024 # Use the same batch size as your training\n",
    "train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "first_batch_indices = train_indices[:BATCH_SIZE]\n",
    "\n",
    "# Run the analysis\n",
    "analyze_batch_label_duplication(dataset, first_batch_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReProver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
