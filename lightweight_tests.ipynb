{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02af156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-18 18:28:11.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlightweight_graph.dataset\u001b[0m:\u001b[36mload_or_create\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mLoading instance-based graph dataset from lightweight_graph/data_instances...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded to cuda:2. Training contexts: 250814\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from lightweight_graph.dataset import LightweightGraphDataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Generator, Tuple\n",
    "\n",
    "\n",
    "# Set PYTHONPATH to project root for the import\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) if \"scripts\" in os.getcwd() or \"notebooks\" in os.getcwd() else os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# --- Configuration ---\n",
    "SAVE_DIR = \"lightweight_graph/data\"\n",
    "# Set the primary device for data loading (e.g., 'cuda:1' or 'cuda:2')\n",
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load and move data to the specified device ---\n",
    "dataset = LightweightGraphDataset.load_or_create(save_dir=SAVE_DIR)\n",
    "dataset.to(DEVICE)\n",
    "\n",
    "print(f\"Dataset loaded to {DEVICE}. Training contexts: {dataset.train_mask.sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d22700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_contexts(\n",
    "    dataset: LightweightGraphDataset,\n",
    "    split_indices: torch.Tensor,\n",
    "    batch_size: int\n",
    ") -> Generator[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Generator that yields mini-batches for GNN training or evaluation.\n",
    "    Each batch contains a subgraph with all premises and a slice of contexts.\n",
    "    \"\"\"\n",
    "    n_premises = dataset.premise_embeddings.size(0)\n",
    "    \n",
    "    # Pre-filter edges and labels for the entire split for efficiency\n",
    "    split_edge_mask = torch.isin(dataset.context_edge_index[1], split_indices)\n",
    "    split_context_edge_index = dataset.context_edge_index[:, split_edge_mask]\n",
    "    split_context_edge_attr = dataset.context_edge_attr[split_edge_mask]\n",
    "    \n",
    "    split_label_mask = torch.isin(dataset.context_premise_labels[0], split_indices)\n",
    "    split_context_premise_labels = dataset.context_premise_labels[:, split_label_mask]\n",
    "\n",
    "    for start in range(0, len(split_indices), batch_size):\n",
    "        end = min(start + batch_size, len(split_indices))\n",
    "        batch_global_indices = split_indices[start:end]\n",
    "        \n",
    "        batch_global_to_local_map = torch.full(\n",
    "            (batch_global_indices.max() + 1,), -1, dtype=torch.long, device=split_indices.device\n",
    "        )\n",
    "        batch_global_to_local_map[batch_global_indices] = torch.arange(\n",
    "            len(batch_global_indices), device=split_indices.device\n",
    "        )\n",
    "        \n",
    "        batch_context_embeddings = dataset.context_embeddings[batch_global_indices]\n",
    "        batch_context_file_indices = dataset.context_to_file_idx_map[batch_global_indices]\n",
    "        \n",
    "        batch_edge_mask = torch.isin(split_context_edge_index[1], batch_global_indices)\n",
    "        batch_context_edge_index_global = split_context_edge_index[:, batch_edge_mask]\n",
    "        batch_context_edge_attr = split_context_edge_attr[batch_edge_mask]\n",
    "        \n",
    "        batch_label_mask = torch.isin(split_context_premise_labels[0], batch_global_indices)\n",
    "        batch_labels_global = split_context_premise_labels[:, batch_label_mask]\n",
    "\n",
    "        # Use the new batch-specific map for shifting indices\n",
    "        batch_context_edge_index = batch_context_edge_index_global.clone()\n",
    "        batch_context_edge_index[1] = batch_global_to_local_map[batch_context_edge_index[1]] + n_premises\n",
    "        \n",
    "        batch_labels = batch_labels_global.clone()\n",
    "        batch_labels[0] = batch_global_to_local_map[batch_labels[0]]\n",
    "\n",
    "        all_batch_embeddings = torch.cat([dataset.premise_embeddings, batch_context_embeddings], dim=0)\n",
    "        all_batch_edge_index = torch.cat([dataset.premise_edge_index, batch_context_edge_index], dim=1)\n",
    "        all_batch_edge_attr = torch.cat([dataset.premise_edge_attr, batch_context_edge_attr], dim=0)\n",
    "\n",
    "        yield all_batch_embeddings, all_batch_edge_index, all_batch_edge_attr, batch_labels, batch_context_file_indices\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Abstract base class for a GNN-based retrieval model.\"\"\"\n",
    "    \n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError(\"Subclasses must implement the training step.\")\n",
    "\n",
    "    def train_epoch(self, dataset: LightweightGraphDataset, batch_size: int) -> float:\n",
    "        train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "        train_generator = batchify_contexts(dataset, train_indices, batch_size)\n",
    "        total_loss, num_batches = 0.0, 0\n",
    "        pbar = tqdm(train_generator, desc=\"Training Epoch\")\n",
    "        for batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, _ in pbar:\n",
    "            loss = self.train_batch(batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels)\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "        return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int) -> torch.Tensor:\n",
    "        raise NotImplementedError(\"Subclasses must implement the prediction logic.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor, batch_context_file_indices: torch.Tensor, dataset: LightweightGraphDataset) -> Dict[str, float]:\n",
    "        n_premises = dataset.premise_embeddings.shape[0]\n",
    "        num_batch_contexts = batch_embeddings.shape[0] - n_premises\n",
    "        scores = self.get_predictions(batch_embeddings, batch_edge_index, batch_edge_attr, num_batch_contexts, n_premises)\n",
    "\n",
    "        # --- Create Accessibility Mask (This logic is now correct) ---\n",
    "        accessible_mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "        for i in range(num_batch_contexts):\n",
    "            context_file_idx = batch_context_file_indices[i].item()\n",
    "            \n",
    "            # 1. Premises in the same file are accessible\n",
    "            in_file_mask = (dataset.premise_to_file_idx_map == context_file_idx)\n",
    "            \n",
    "            # 2. Premises in imported files (transitive) are accessible\n",
    "            # This is a single lookup because file_dependency_edge_index IS the transitive closure.\n",
    "            dependency_file_indices = dataset.file_dependency_edge_index[1, dataset.file_dependency_edge_index[0] == context_file_idx]\n",
    "            imported_mask = torch.isin(dataset.premise_to_file_idx_map, dependency_file_indices)\n",
    "            \n",
    "            accessible_mask[i] = in_file_mask | imported_mask\n",
    "        \n",
    "        scores.masked_fill_(~accessible_mask, -torch.inf)\n",
    "        \n",
    "        # --- Metric Calculation ---\n",
    "        gt_mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "        gt_mask[batch_labels[0], batch_labels[1]] = True\n",
    "        num_positives = gt_mask.sum(dim=1)\n",
    "        valid_contexts = num_positives > 0\n",
    "        if not valid_contexts.any(): return {'R@1': 0.0, 'R@10': 0.0, 'MRR': 0.0}\n",
    "\n",
    "        top_10_indices = scores.topk(k=10, dim=1).indices\n",
    "        top_10_hits = gt_mask.gather(1, top_10_indices)\n",
    "\n",
    "        recall_at_1 = (top_10_hits[:, 0][valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "        recall_at_10 = (top_10_hits.sum(dim=1)[valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "        \n",
    "        sorted_indices = scores.argsort(dim=1, descending=True)\n",
    "        sorted_gt = gt_mask.gather(1, sorted_indices)\n",
    "        first_hit_rank = torch.argmax(sorted_gt[valid_contexts].int(), dim=1) + 1\n",
    "        mrr = (1.0 / first_hit_rank).mean().item()\n",
    "        \n",
    "        return {'R@1': recall_at_1, 'R@10': recall_at_10, 'MRR': mrr}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self, dataset: LightweightGraphDataset, split: str, batch_size: int) -> Dict[str, float]:\n",
    "        mask = getattr(dataset, f\"{split}_mask\", None)\n",
    "        if mask is None: raise ValueError(f\"Invalid split: {split}\")\n",
    "        \n",
    "        split_indices = mask.nonzero(as_tuple=True)[0]\n",
    "        eval_generator = batchify_contexts(dataset, split_indices, batch_size)\n",
    "        \n",
    "        all_metrics = []\n",
    "        pbar = tqdm(eval_generator, desc=f\"Evaluating on {split} split\")\n",
    "        for batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, batch_context_file_indices in pbar:\n",
    "            metrics = self.eval_batch(batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, batch_context_file_indices, dataset)\n",
    "            all_metrics.append(metrics)\n",
    "            pbar.set_postfix(metrics)\n",
    "\n",
    "        if not all_metrics: return {'R@1': 0.0, 'R@10': 0.0, 'MRR': 0.0}\n",
    "        \n",
    "        final_metrics = {key: torch.tensor([m[key] for m in all_metrics]).mean().item() for key in all_metrics[0]}\n",
    "        \n",
    "        print(f\"\\n--- Evaluation Results for '{split}' split ---\")\n",
    "        print(f\"  Recall@1:  {final_metrics['R@1']:.4f}\")\n",
    "        print(f\"  Recall@10: {final_metrics['R@10']:.4f}\")\n",
    "        print(f\"  MRR:       {final_metrics['MRR']:.4f}\")\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "        return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd0b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Initialized BaselineModel (no GNN, no training).\")\n",
    "\n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor) -> float:\n",
    "        return 0.0\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int) -> torch.Tensor:\n",
    "        # The input batch_embeddings are the initial LM embeddings.\n",
    "        # We simply ignore the edge_index and edge_attr.\n",
    "        initial_premise_embs = batch_embeddings[:n_premises]\n",
    "        initial_context_embs = batch_embeddings[n_premises:]\n",
    "\n",
    "        # L2-normalize for cosine similarity calculation.\n",
    "        premise_embs_norm = F.normalize(initial_premise_embs, p=2, dim=1)\n",
    "        context_embs_norm = F.normalize(initial_context_embs, p=2, dim=1)\n",
    "\n",
    "        # Compute similarity scores via matrix multiplication.\n",
    "        scores = torch.mm(context_embs_norm, premise_embs_norm.T)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7914568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBaselineModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Initialized RandomBaselineModel (random scores, no training).\")\n",
    "\n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor) -> float:\n",
    "        return 0.0\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int) -> torch.Tensor:\n",
    "        # Generate random scores for each context-premise pair.\n",
    "        scores = torch.rand((num_batch_contexts, n_premises), device=batch_embeddings.device)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d12073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline_model = BaselineModel()\n",
    "#baseline_val_metrics = baseline_model.eval(dataset, split=\"train\", batch_size=5120)\n",
    "#baseline_val_metrics = baseline_model.eval(dataset, split=\"val\", batch_size=256)\n",
    "#baseline_test_metrics = baseline_model.eval(dataset, split=\"test\", batch_size=256)\n",
    "\n",
    "#random_model = RandomBaselineModel()\n",
    "#random_val_metrics = random_model.eval(dataset, split=\"train\", batch_size=5120)\n",
    "#random_val_metrics = random_model.eval(dataset, split=\"val\", batch_size=256)\n",
    "#random_test_metrics = random_model.eval(dataset, split=\"test\", batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from numpy import negative\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from torch_geometric.utils import dropout_edge\n",
    "from typing import Tuple, Literal\n",
    "\n",
    "# from your_abstract_model_file import Model\n",
    "# from lightweight_graph.dataset import LightweightGraphDataset\n",
    "\n",
    "def calculate_metrics(scores: torch.Tensor, gt_mask: torch.Tensor) -> float:\n",
    "    num_positives = gt_mask.sum(dim=1)\n",
    "    valid_contexts = num_positives > 0\n",
    "    if not valid_contexts.any():\n",
    "        return {\"R@1\": 0.0, \"R@10\": 0.0, \"MRR\": 0.0}\n",
    "\n",
    "    # calculate R@1, R@10, MRR\n",
    "    top_10_indices = scores.topk(k=10, dim=1).indices\n",
    "    top_10_hits = gt_mask.gather(1, top_10_indices)\n",
    "\n",
    "    tr1 = (torch.ones_like(top_10_hits)[:, 0][valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "    tr10 = (torch.ones_like(top_10_hits).sum(dim=1)[valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "\n",
    "    recall_at_1 = (top_10_hits[:, 0][valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "    recall_at_10 = (top_10_hits.sum(dim=1)[valid_contexts] / num_positives[valid_contexts]).mean().item()\n",
    "\n",
    "    # compute reciprocal rank\n",
    "    #ranks = torch.arange(1, 11, device=scores.device).float()  # [1,2,...,10]\n",
    "    #reciprocal_ranks = (top_10_hits * (1.0 / ranks)).max(dim=1).values\n",
    "    #mrr = reciprocal_ranks[valid_contexts].mean().item()\n",
    "\n",
    "    return {\"R@1\": recall_at_1, \"R@10\": recall_at_10, \"R@1 upper bound\" : tr1, \"R@10 upper bound\" : tr10}#, \"MRR\": mrr}\n",
    "\n",
    "class HeadAttentionScoring(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, aggregation: Literal[\"logsumexp\", \"mean\", \"max\", \"gated\", \"nn\"]):\n",
    "        super(HeadAttentionScoring, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.score_W = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "        if aggregation == \"gated\":\n",
    "            self.gate_W = nn.Linear(embedding_dim, num_heads)\n",
    "        if aggregation == \"nn\":\n",
    "            self.nn = nn.Sequential(\n",
    "                nn.Linear(2 * embedding_dim, embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(embedding_dim, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, premise_embs: torch.Tensor, context_embs: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = context_embs.size(0)\n",
    "        n_premises = premise_embs.size(0)\n",
    "\n",
    "        if self.aggregation == \"nn\":\n",
    "            # Expand context embeddings to match premise embeddings\n",
    "            context_expanded = context_embs.unsqueeze(1).expand(-1, n_premises, -1)  # (batch_size, n_premises, embedding_dim)\n",
    "            premise_expanded = premise_embs.unsqueeze(0).expand(batch_size, -1, -1)  # (batch_size, n_premises, embedding_dim)\n",
    "            combined = torch.cat([context_expanded, premise_expanded], dim=-1)  # (batch_size, n_premises, 2 * embedding_dim)\n",
    "            scores = self.nn(combined).squeeze(-1)  # (batch_size, n_premises)\n",
    "            return scores  # (batch_size, n_premises)\n",
    "\n",
    "        context_embs = self.score_W(context_embs)  # (batch_size, embedding_dim)\n",
    "        # reshape for multi-head\n",
    "        context_embs = context_embs.view(batch_size, self.num_heads, self.embedding_dim // self.num_heads)  # (batch_size, num_heads, head_dim)\n",
    "        premise_embs = premise_embs.view(n_premises, self.num_heads, self.embedding_dim // self.num_heads)  # (n_premises, num_heads, head_dim)\n",
    "        # compute attention scores to get (batch_size, n_premises, num_heads)\n",
    "        scores = torch.einsum('bhd, phd -> bhp', context_embs, premise_embs)  # (batch_size, num_heads, n_premises)\n",
    "        scores = scores.permute(0, 2, 1)  # (batch_size, n_premises, num_heads)\n",
    "\n",
    "        if self.aggregation == \"max\":\n",
    "            scores, _ = scores.max(dim=-1)  # (batch_size, n_premises)\n",
    "        elif self.aggregation == \"mean\":\n",
    "            scores = scores.mean(dim=-1)  # (batch_size, n_premises)\n",
    "        elif self.aggregation == \"logsumexp\":\n",
    "            scores = torch.logsumexp(scores, dim=-1)  # (batch_size, n_premises)\n",
    "        elif self.aggregation == \"gated\":\n",
    "            score_gates = self.gate_W(context_embs.mean(dim=1))  # (batch_size, num_heads)\n",
    "            # apply softmax to get weights\n",
    "            score_gates = F.softmax(score_gates, dim=-1)  # (batch_size, num_heads)\n",
    "            scores = (scores * score_gates.unsqueeze(1)).sum(dim=-1)  # (batch_size, n_premises)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregation method: {self.aggregation}\")\n",
    "\n",
    "        return scores  # (batch_size, n_premises)\n",
    "\n",
    "class TestModel(Model, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: LightweightGraphDataset,\n",
    "        hidden_dim: int,\n",
    "        aggregation: Literal[\"mean\", \"max\", \"logsumexp\", \"gated\", \"nn\"],\n",
    "        n_heads: int,\n",
    "        lr: float = 1e-4,\n",
    "        loss : Literal[\"bce\", \"mse\"] = \"mse\",\n",
    "    ):\n",
    "        Model.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.embedding_dim = dataset.premise_embeddings.shape[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_relations = len(dataset.edge_types_map)\n",
    "        \n",
    "        self.random_premise_embeds = nn.Embedding(dataset.premise_embeddings.shape[0], self.hidden_dim)\n",
    "        self.random_premise_embed_for_context = nn.Embedding(dataset.premise_embeddings.shape[0], self.embedding_dim)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
    "        # Keep track of metrics during hard mining\n",
    "        self.last_hard_mining_recall = 0.0\n",
    "        self.loss = loss\n",
    "\n",
    "        self.rgcn = RGCNConv(\n",
    "            in_channels=self.embedding_dim,\n",
    "            out_channels=self.hidden_dim,\n",
    "            num_relations=2,\n",
    "        )\n",
    "\n",
    "        self.scoring = HeadAttentionScoring(embedding_dim=self.hidden_dim, num_heads=n_heads, aggregation=aggregation)\n",
    "        \n",
    "        print(f\"Initialized RGCNModel with {self.num_relations} relations, hidden_dim={self.hidden_dim}\")\n",
    "\n",
    "    def forward(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, n_premises: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        expected_dtype = torch.float32\n",
    "        initial_premise_embs = self.random_premise_embeds.weight.to(expected_dtype)\n",
    "        \n",
    "        initial_context_embs = torch.zeros_like(batch_embeddings[n_premises:]).to(expected_dtype)\n",
    "        initial_premise_emb_for_context = self.random_premise_embed_for_context.weight.to(expected_dtype)\n",
    "        batch_embeddings_for_context = torch.cat([initial_premise_emb_for_context, initial_context_embs], dim=0)\n",
    "\n",
    "        print(f\"memory before rgcn: {torch.cuda.memory_allocated(DEVICE)/1e9 if torch.cuda.is_available() else 0.0} GB\")\n",
    "        refined_context_embs = self.rgcn(batch_embeddings_for_context, batch_edge_index, batch_edge_attr)\n",
    "        print(f\"memory after rgcn: {torch.cuda.memory_allocated(DEVICE)/1e9 if torch.cuda.is_available() else 0.0} GB\")\n",
    "        \n",
    "        # refine the premise embeddings first\n",
    "        return initial_premise_embs, refined_context_embs[n_premises:]\n",
    "\n",
    "    def get_predictions(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, num_batch_contexts: int, n_premises: int, squash01 : bool = True) -> torch.Tensor:\n",
    "        final_premise_embs, final_context_embs = self.forward(batch_embeddings, batch_edge_index, batch_edge_attr, n_premises)\n",
    "        return self.scoring.forward(final_premise_embs, final_context_embs)\n",
    "\n",
    "    def train_batch(self, batch_embeddings: torch.Tensor, batch_edge_index: torch.Tensor, batch_edge_attr: torch.Tensor, batch_labels: torch.Tensor, i) -> torch.Tensor:\n",
    "        self.train()\n",
    "        n_premises = self.premise_embeddings_shape[0]\n",
    "        num_batch_contexts = batch_embeddings.shape[0] - n_premises\n",
    "        logits_tensor = self.get_predictions(batch_embeddings, batch_edge_index, batch_edge_attr, num_batch_contexts, n_premises, squash01 = False)\n",
    "        \n",
    "        targets_tensor = torch.zeros_like(logits_tensor)\n",
    "        pos_context_indices = batch_labels[0]\n",
    "        pos_premise_indices = batch_labels[1]\n",
    "        targets_tensor[pos_context_indices, pos_premise_indices] = 1.0\n",
    "\n",
    "        # report R@10 during training for monitoring\n",
    "        self.last_metrics = calculate_metrics(logits_tensor, targets_tensor)\n",
    "        \n",
    "        # --- Weighted Loss Calculation (works for all strategies) ---\n",
    "        n_negative = (targets_tensor == 0).sum().item()\n",
    "        n_positive = (targets_tensor == 1).sum().item()\n",
    "\n",
    "        if n_positive == 0 or n_negative == 0:\n",
    "            # Handle edge case where we have only one class\n",
    "            weights = torch.ones_like(logits_tensor)\n",
    "        else:\n",
    "            # Calculate class weights to balance the loss\n",
    "            pos_weight = n_negative / n_positive  # Higher weight for minority class\n",
    "            weights = torch.ones_like(logits_tensor)\n",
    "            weights[targets_tensor == 1] = pos_weight\n",
    "\n",
    "        if self.loss == \"bce\":\n",
    "            unweighted_loss = F.binary_cross_entropy_with_logits(logits_tensor, targets_tensor, reduction='none')\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function: {self.loss}\")\n",
    "        weighted_loss = (unweighted_loss * weights).mean()\n",
    "\n",
    "        return weighted_loss\n",
    "\n",
    "    def train_epoch(self, dataset: LightweightGraphDataset, batch_size: int, accumulation_steps: int = 1) -> float:\n",
    "        self.premise_embeddings_shape = dataset.premise_embeddings.shape\n",
    "        train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "        train_generator = batchify_contexts(dataset, train_indices, batch_size)\n",
    "        \n",
    "        total_loss, num_batches_processed = 0.0, 0\n",
    "        self.optimizer.zero_grad()\n",
    "        pbar = tqdm(enumerate(train_generator), total=len(train_indices)//batch_size, desc=\"Training Epoch\")\n",
    "        for i, (batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, _) in pbar:\n",
    "            if (i !=0):\n",
    "                assert 0\n",
    "            for j in range(10000):\n",
    "                loss = self.train_batch(batch_embeddings, batch_edge_index, batch_edge_attr, batch_labels, j)\n",
    "                loss = loss / accumulation_steps\n",
    "                memory = torch.cuda.memory_allocated(DEVICE)/1e9 if torch.cuda.is_available() else 0.0\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "                log_dict = {\"loss\": f\"{loss.item() * accumulation_steps:.4f}\", \"memory (GB)\": f\"{memory}\"}\n",
    "                log_dict.update(self.last_metrics)\n",
    "                pbar.set_postfix(log_dict)\n",
    "\n",
    "                total_loss += loss.item() * accumulation_steps\n",
    "                num_batches_processed += 1\n",
    "                \n",
    "        if num_batches_processed % accumulation_steps != 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        return total_loss / num_batches_processed if num_batches_processed > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a662e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RGCNModel with 2 relations, hidden_dim=512\n",
      "\n",
      "--- [\"All\" Negatives] Epoch 1/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/15675 [00:01<?, ?it/s, loss=1.3792, memory (GB)=24.741586944, R@1=0, R@10=0, R@1 upper bound=0.556, R@10 upper bound=5.56]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.52 GiB. GPU 2 has a total capacity of 47.41 GiB of which 4.97 GiB is free. Including non-PyTorch memory, this process has 42.42 GiB memory in use. Of the allocated memory 31.22 GiB is allocated by PyTorch, and 10.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- [\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mAll\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m Negatives] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     avg_loss = testmodel.train_epoch(dataset, batch_size=BATCH_SIZE, accumulation_steps=ACCUMULATION_STEPS)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnd of Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Average Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Evaluate on validation set after each epoch\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mTestModel.train_epoch\u001b[39m\u001b[34m(self, dataset, batch_size, accumulation_steps)\u001b[39m\n\u001b[32m    191\u001b[39m loss = loss / accumulation_steps\n\u001b[32m    192\u001b[39m memory = torch.cuda.memory_allocated(DEVICE)/\u001b[32m1e9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m loss.backward()\n\u001b[32m    194\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m torch.autograd.backward(\n\u001b[32m    649\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    650\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m _engine_run_backward(\n\u001b[32m    354\u001b[39m     tensors,\n\u001b[32m    355\u001b[39m     grad_tensors_,\n\u001b[32m    356\u001b[39m     retain_graph,\n\u001b[32m    357\u001b[39m     create_graph,\n\u001b[32m    358\u001b[39m     inputs,\n\u001b[32m    359\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    360\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ReProver/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    825\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    826\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 5.52 GiB. GPU 2 has a total capacity of 47.41 GiB of which 4.97 GiB is free. Including non-PyTorch memory, this process has 42.42 GiB memory in use. Of the allocated memory 31.22 GiB is allocated by PyTorch, and 10.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 1e-2\n",
    "# Note: For \"all\" strategy, a smaller batch size is needed due to the large logit matrix\n",
    "BATCH_SIZE = 16\n",
    "ACCUMULATION_STEPS = 1 # Effective batch size = 256 * 16 = 4096\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "# --- Instantiate the Model ---\n",
    "testmodel = TestModel(\n",
    "    dataset=dataset, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    lr=LEARNING_RATE,\n",
    "    aggregation=\"nn\",\n",
    "    n_heads=8,\n",
    "    loss=\"bce\",\n",
    ")\n",
    "testmodel.to(DEVICE)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- [\\\"All\\\" Negatives] Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    avg_loss = testmodel.train_epoch(dataset, batch_size=BATCH_SIZE, accumulation_steps=ACCUMULATION_STEPS)\n",
    "    print(f\"End of Epoch {epoch+1}, Average Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set after each epoch\n",
    "    testmodel.eval(dataset, split=\"val\", batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Batch Label Duplication ---\n",
      "Total positive labels in batch: 1745\n",
      "Unique premises to be retrieved: 913\n",
      "\n",
      "--- Duplication Stats ---\n",
      "Number of unique premises that are duplicated: 353 (out of 913)\n",
      "Percentage of unique premises that are duplicated: 38.66%\n",
      "Total labels pointing to duplicated premises: 1185 (out of 1745)\n",
      "Percentage of labels that are for duplicated premises: 67.91%\n",
      "\n",
      "This means 832 times, the model must reuse a single premise embedding for different contexts.\n",
      "On average, a duplicated premise is required 3.36 times within this batch.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# %% Cell to Analyze Label Duplication in a Batch\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_batch_label_duplication(\n",
    "    dataset: LightweightGraphDataset,\n",
    "    batch_global_indices: torch.Tensor\n",
    "):\n",
    "    print(\"--- Analyzing Batch Label Duplication ---\")\n",
    "\n",
    "    split_label_mask = torch.isin(dataset.context_premise_labels[0], batch_global_indices)\n",
    "    batch_labels_global = dataset.context_premise_labels[:, split_label_mask]\n",
    "\n",
    "    if batch_labels_global.shape[1] == 0:\n",
    "        print(\"No positive labels found in this batch.\")\n",
    "        return\n",
    "\n",
    "    positive_premise_indices = batch_labels_global[1]\n",
    "    unique_premises, counts = torch.unique(positive_premise_indices, return_counts=True)\n",
    "    \n",
    "    total_labels = len(positive_premise_indices)\n",
    "    num_unique_premises = len(unique_premises)\n",
    "    \n",
    "    print(f\"Total positive labels in batch: {total_labels}\")\n",
    "    print(f\"Unique premises to be retrieved: {num_unique_premises}\")\n",
    "\n",
    "    duplicated_mask = counts > 1\n",
    "    num_duplicated_premises = duplicated_mask.sum().item()\n",
    "    \n",
    "    if num_duplicated_premises == 0:\n",
    "        print(\"No premise is a correct label for more than one context in this batch.\")\n",
    "        return\n",
    "\n",
    "    duplicated_premise_ids = unique_premises[duplicated_mask]\n",
    "    duplicated_premise_counts = counts[duplicated_mask]\n",
    "    \n",
    "    total_labels_involved_in_duplication = duplicated_premise_counts.sum().item()\n",
    "\n",
    "    num_redundant_labels = (duplicated_premise_counts - 1).sum().item()\n",
    "\n",
    "    avg_duplication_factor = duplicated_premise_counts.float().mean().item()\n",
    "    \n",
    "    print(f\"\\n--- Duplication Stats ---\")\n",
    "    print(f\"Number of unique premises that are duplicated: {num_duplicated_premises} (out of {num_unique_premises})\")\n",
    "    print(f\"Percentage of unique premises that are duplicated: {num_duplicated_premises / num_unique_premises:.2%}\")\n",
    "    print(f\"Total labels pointing to duplicated premises: {total_labels_involved_in_duplication} (out of {total_labels})\")\n",
    "    print(f\"Percentage of labels that are for duplicated premises: {total_labels_involved_in_duplication / total_labels:.2%}\")\n",
    "    print(f\"\\nThis means {num_redundant_labels} times, the model must reuse a single premise embedding for different contexts.\")\n",
    "    print(f\"On average, a duplicated premise is required {avg_duplication_factor:.2f} times within this batch.\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "# --- How to use it ---\n",
    "# Get the first batch of training indices to test\n",
    "train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "first_batch_indices = train_indices[:BATCH_SIZE]\n",
    "\n",
    "# Run the analysis\n",
    "analyze_batch_label_duplication(dataset, first_batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell to Analyze Label Duplication in a Batch\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_batch_label_duplication(\n",
    "    dataset: LightweightGraphDataset,\n",
    "    batch_global_indices: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes the duplication of ground-truth premises within a single batch.\n",
    "\n",
    "    This helps quantify the \"representation bottleneck\" problem, where the model\n",
    "    must learn a single embedding for a premise that needs to be retrieved by\n",
    "    multiple, different contexts in the same batch.\n",
    "    \"\"\"\n",
    "    print(\"--- Analyzing Batch Label Duplication ---\")\n",
    "\n",
    "    # 1. Find all labels that are relevant to this specific batch of contexts.\n",
    "    # This logic is borrowed from the batchify_contexts generator.\n",
    "    split_label_mask = torch.isin(dataset.context_premise_labels[0], batch_global_indices)\n",
    "    batch_labels_global = dataset.context_premise_labels[:, split_label_mask]\n",
    "\n",
    "    if batch_labels_global.shape[1] == 0:\n",
    "        print(\"No positive labels found in this batch.\")\n",
    "        return\n",
    "\n",
    "    # 2. Isolate the premise indices from the labels. These are the items being retrieved.\n",
    "    positive_premise_indices = batch_labels_global[1]\n",
    "\n",
    "    # 3. Count the occurrences of each unique premise index.\n",
    "    unique_premises, counts = torch.unique(positive_premise_indices, return_counts=True)\n",
    "    \n",
    "    total_labels = len(positive_premise_indices)\n",
    "    num_unique_premises = len(unique_premises)\n",
    "    \n",
    "    print(f\"Total positive labels in batch: {total_labels}\")\n",
    "    print(f\"Unique premises to be retrieved: {num_unique_premises}\")\n",
    "\n",
    "    # 4. Identify the duplicated premises and quantify the duplication.\n",
    "    duplicated_mask = counts > 1\n",
    "    num_duplicated_premises = duplicated_mask.sum().item()\n",
    "    \n",
    "    if num_duplicated_premises == 0:\n",
    "        print(\"No premise is a correct label for more than one context in this batch.\")\n",
    "        return\n",
    "\n",
    "    # 5. Calculate statistics on the duplicates.\n",
    "    duplicated_premise_ids = unique_premises[duplicated_mask]\n",
    "    duplicated_premise_counts = counts[duplicated_mask]\n",
    "    \n",
    "    # This is the total number of labels that point to a premise that is needed more than once.\n",
    "    # For example, if premise P is needed 3 times, it contributes 3 to this sum.\n",
    "    total_labels_involved_in_duplication = duplicated_premise_counts.sum().item()\n",
    "\n",
    "    # This is the number of \"extra\" pulls on the same embedding.\n",
    "    # If premise P is needed 3 times, it has 2 \"extra\" pulls.\n",
    "    num_redundant_labels = (duplicated_premise_counts - 1).sum().item()\n",
    "\n",
    "    avg_duplication_factor = duplicated_premise_counts.float().mean().item()\n",
    "    \n",
    "    print(f\"\\n--- Duplication Stats ---\")\n",
    "    print(f\"Number of unique premises that are duplicated: {num_duplicated_premises} (out of {num_unique_premises})\")\n",
    "    print(f\"Percentage of unique premises that are duplicated: {num_duplicated_premises / num_unique_premises:.2%}\")\n",
    "    print(f\"Total labels pointing to duplicated premises: {total_labels_involved_in_duplication} (out of {total_labels})\")\n",
    "    print(f\"Percentage of labels that are for duplicated premises: {total_labels_involved_in_duplication / total_labels:.2%}\")\n",
    "    print(f\"\\nThis means {num_redundant_labels} times, the model must reuse a single premise embedding for different contexts.\")\n",
    "    print(f\"On average, a duplicated premise is required {avg_duplication_factor:.2f} times within this batch.\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "# --- How to use it ---\n",
    "# Get the first batch of training indices to test\n",
    "BATCH_SIZE = 1024 # Use the same batch size as your training\n",
    "train_indices = dataset.train_mask.nonzero(as_tuple=True)[0]\n",
    "first_batch_indices = train_indices[:BATCH_SIZE]\n",
    "\n",
    "# Run the analysis\n",
    "analyze_batch_label_duplication(dataset, first_batch_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReProver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
