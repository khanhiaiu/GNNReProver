# In retrieval/confs/cli_gnn.yaml

seed_everything: 3407
trainer:
  accelerator: gpu
  devices: 1
  precision: 32-true
  strategy:
    class_path: pytorch_lightning.strategies.DeepSpeedStrategy
    init_args:
      stage: 2
      offload_optimizer: false
      cpu_checkpointing: false
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      project: reprover-gnn
      name: gnn_retriever1 # This will be overridden by the script's EXP_NAME
      save_dir: "lightning_logs/gnn_retriever" # This will be overridden by the script's LOG_DIR
  gradient_clip_val: 1.0
  max_steps: 20000 # A reasonable number for a full training run. Lower for quick tests.
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        verbose: true
        save_top_k: 1
        save_last: true
        monitor: loss_train_epoch # GNN model does not have a val step, so we monitor train loss.
        mode: min

model:
  feature_size: 1472 # ByT5-small embedding dimension. Must match the retriever.
  num_layers: 2
  lr: 1e-4
  warmup_steps: 1000
  loss_function: mse # Options: "mse", "cross_entropy"
  l1_lambda: 0.0  # L1 regularization coefficient. Set to 0.0 to disable.
  weight_decay: 5e-4 # L2 regularization (weight decay). Set to 0.0 to disable.
  edge_type_to_id: {}  # This is populated dynamically by the training script.
  
  # --- GNN Architecture ---
  gnn_layer_type: rgcn  # Options: "gcn", "rgcn", "gat", "rgat", "gin", "graphsage"
  num_relations: 3      # Must match the number of edge types produced by the graph_dependencies_config.
                        # For this config ('clickable', distinguish_lctx_goal=true), it's 2.
  hidden_size: 768    # Hidden layer size. Can be used to reduce dimensionality.
  use_residual: true    # Whether to use residual/skip connections.
  dropout_p: 0.3        # Dropout probability within the GNN.
  edge_dropout_p: 0.1   # Dropout probability for edges (a form of graph augmentation).
  norm_type: layer      # Normalization type: "none", "batch", "layer".
  use_initial_projection: true # Whether to apply a linear layer to input embeddings before the GNN.
  
  # --- Post-processing ---
  # How to use the GNN-refined embedding.
  # "gnn": Use ONLY the GNN's final output embedding.
  # "concat": Concatenate the GNN embedding with the original text embedding.
  # "gating": Learn a gate to fuse the GNN and original text embeddings.
  postprocess_gnn_embeddings: "gnn"

  # --- Training Speed Optimization ---
  # To disable, set to `null`. To enable, provide a list of integers (e.g., [15, 10]).
  neighbor_sampling_sizes: null

data:
  data_path: data/leandojo_benchmark_4/random/
  corpus_path: data/leandojo_benchmark_4/corpus.jsonl
  retriever_ckpt_path: kaiyuy/leandojo-lean4-retriever-byt5-small
  
  # --- Batching and Workers ---
  # A larger batch size is generally better for the 'random' negative sampling strategy.
  batch_size: 512
  eval_batch_size: 4 # Used for pre-calculating embeddings.
  num_workers: 6
  
  # --- Negative Mining Strategy ---
  # Defines how negative examples are chosen for the contrastive loss.
  negative_mining:
    strategy: 'hard'        # 'random' for pre-sampled random negatives. 'hard' is also an option.
    num_negatives: 3         # Total number of negative examples per positive one.
    num_in_file_negatives: 1  # How many of the negatives should be sampled from the same file (if available).

  # --- Graph Construction ---
  # This config determines which dependencies are turned into graph edges.
  graph_dependencies_config:
    mode: 'custom'                 # 'all_dojo' or 'custom'
    use_proof_dependencies: false  # Set to true to add edges for proof dependencies.
    signature_and_state:
      verbosity: 'clickable'       # How premise names are extracted. 'clickable' is generally best.
      distinguish_lctx_goal: true  # Create separate edge types for local context vs. goal premises.

  # This key is not used in the GNN model yet but is good practice to include
  attributes:
    use_node_attributes: true