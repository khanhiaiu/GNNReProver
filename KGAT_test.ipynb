{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29f963",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1250243143.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef __init__(self, config):\u001b[39m\n                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "\n",
    "class KGAT(torch.nn.Module):\n",
    "    def __init__(self, config : dict[str, Any]):\n",
    "        super(KGAT, self).__init__() \n",
    "        self.config = config\n",
    "\n",
    "        self.premise_embeddings = torch.nn.Embedding(config['num_premises'], config['embedding_dim'])\n",
    "\n",
    "        self.relations_weights = torch.nn.ModuleList()\n",
    "        \n",
    "        for _ in range(config['n_layers']):\n",
    "            self.relations_weights.append(\n",
    "                torch.nn.Parameter(torch.Tensor(config['num_relations'], config['embedding_dim'], config['embedding_dim']))\n",
    "            )\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.premise_embeddings.weight)\n",
    "        for rel_weight in self.relations_weights:\n",
    "            torch.nn.init.xavier_uniform_(rel_weight)\n",
    "\n",
    "        self.scorer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(config['embedding_dim'] * (config['n_layers'] + 1) * 2, config['embedding_dim']),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(config['embedding_dim'], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, n_contexts : int, edge_index : torch.Tensor, edge_type : torch.Tensor):\n",
    "        premise_emb = self.premise_embeddings.weight\n",
    "    \n",
    "        # append zeros for n_contexts\n",
    "        context_emb = torch.zeros((n_contexts, self.config['embedding_dim']), device=premise_emb.device)\n",
    "        X = torch.cat([premise_emb, context_emb], dim=0)\n",
    "\n",
    "        all_embs = [X]\n",
    "        for layer in range(self.config['n_layers']):\n",
    "            neigh_emb = torch.zeros_like(X)\n",
    "            for rel in range(self.config['num_relations']):\n",
    "                rel_edges = (edge_type == rel).nonzero(as_tuple=True)[0]\n",
    "                if rel_edges.numel() == 0:\n",
    "                    continue\n",
    "                src_nodes = edge_index[0, rel_edges]\n",
    "                dst_nodes = edge_index[1, rel_edges]\n",
    "                \n",
    "                # move both head and tail to relation subspace\n",
    "                src_emb = X[src_nodes]\n",
    "                rel_weight = self.relations_weights[layer][rel]\n",
    "                src_transformed = torch.matmul(src_emb, rel_weight)\n",
    "                dst_emb = X[dst_nodes]\n",
    "                dst_transformerd = torch.tanh(torch.matmul(dst_emb, rel_weight))\n",
    "                # attention scores\n",
    "                scores = (src_transformed * dst_transformerd).sum(dim=1)\n",
    "                messages = src_transformed * scores.unsqueeze(1)\n",
    "\n",
    "                # apply message dropout\n",
    "                messages = torch.nn.functional.dropout(messages, p=self.config['message_dropout'], training=self.training)\n",
    "    \n",
    "                # aggregate messages using add\n",
    "                neigh_emb = neigh_emb.index_add(0, dst_nodes, messages)\n",
    "                \n",
    "\n",
    "            #biinteraction=LeakyReLU W1(eh + eN) +  LeakyReLU W2(eh âŠ™ eN),\n",
    "            X = torch.nn.functional.leaky_relu(X + neigh_emb) + torch.nn.functional.leaky_relu(X * neigh_emb)\n",
    "            # normalize\n",
    "            X = torch.nn.functional.dropout(X, p=self.config['node_dropout'], training=self.training)\n",
    "            X = torch.nn.functional.normalize(X, p=2, dim=1)\n",
    "\n",
    "            all_embs.append(X)\n",
    "\n",
    "        all_embs = torch.cat(all_embs, dim=1) # (num_premises + n_contexts, embedding_dim * (n_layers + 1))\n",
    "\n",
    "        return all_embs[:self.config['num_premises']], all_embs[self.config['num_premises']:] # premise_emb, context_emb\n",
    "    \n",
    "    def score(self, n_contexts : int, edge_index : torch.Tensor, edge_type : torch.Tensor) -> torch.Tensor:\n",
    "        premise_emb, context_emb = self.forward(n_contexts, edge_index, edge_type)\n",
    "        n_premises = premise_emb.size(0)\n",
    "\n",
    "        # create all pairs of premise and context embeddings\n",
    "        premise_expanded = premise_emb.unsqueeze(1).expand(-1, n_contexts, -1)\n",
    "        context_expanded = context_emb.unsqueeze(0).expand(n_premises, -1, -1)\n",
    "\n",
    "        # concatenate premise and context embeddings\n",
    "        pair_emb = torch.cat([premise_expanded, context_expanded], dim=-1) # (n_premises, n_contexts, embedding_dim * (n_layers + 1) * 2)\n",
    "\n",
    "        scores = self.scorer(pair_emb).squeeze(-1) # (n_premises, n_contexts)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# made up training data for testing one batch overfitting\n",
    "NUM_PREMISES = 10000\n",
    "NUM_CONTEXTS = 100\n",
    "NUM_RELATIONS = 2\n",
    "PREMISE_EDGE_INDEX = torch.randint(0, NUM_PREMISES, (2, 100000)).cuda()\n",
    "# Remove edges where src >= dst\n",
    "PREMISE_EDGE_INDEX = PREMISE_EDGE_INDEX[:, PREMISE_EDGE_INDEX[0] < PREMISE_EDGE_INDEX[1]]\n",
    "PREMISE_EDGE_TYPE = torch.randint(0, NUM_RELATIONS, (PREMISE_EDGE_INDEX.size(1),)).cuda()\n",
    "\n",
    "PREMISE_TO_CONTEXT_EDGE_INDEX = torch.concat([\n",
    "    torch.randint(0, NUM_PREMISES, (1, 5000)),\n",
    "    torch.randint(NUM_PREMISES, NUM_PREMISES + NUM_CONTEXTS, (1, 5000))\n",
    "], dim=0).cuda()\n",
    "PREMISE_TO_CONTEXT_EDGE_TYPE = torch.randint(0, NUM_RELATIONS, (PREMISE_TO_CONTEXT_EDGE_INDEX.size(1),)).cuda()\n",
    "\n",
    "CONTEXT_LABELS = torch.concat([\n",
    "    torch.randint(NUM_PREMISES, NUM_PREMISES + NUM_CONTEXTS, (2 * NUM_CONTEXTS,)).cuda(),\n",
    "    torch.randint(0, NUM_PREMISES, (2 * NUM_CONTEXTS,)).cuda()\n",
    "])\n",
    "\n",
    "EDGE_INDEX = torch.concat([PREMISE_EDGE_INDEX, PREMISE_TO_CONTEXT_EDGE_INDEX], dim=1)\n",
    "EDGE_TYPE = torch.concat([PREMISE_EDGE_TYPE, PREMISE_TO_CONTEXT_EDGE_TYPE], dim=0)\n",
    "\n",
    "config = {\n",
    "    'num_premises': NUM_PREMISES,\n",
    "    'num_relations': NUM_RELATIONS,\n",
    "    'embedding_dim': 64,\n",
    "    'n_layers': 2,\n",
    "    'message_dropout': 0.1,\n",
    "    'node_dropout': 0.1,\n",
    "}\n",
    "\n",
    "model = KGAT(config).cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "model.train()\n",
    "\n",
    "def Rat10(scores : torch.Tensor, labels : torch.Tensor) -> float:\n",
    "    recommended_indices = torch.topk(scores, k=10, dim=1).indices\n",
    "    is_correct = labels[recommended_indices]\n",
    "    return is_correct.sum().item() / torch.max(labels.sum(dim=1), 10).sum()\n",
    "\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    scores = model.score(NUM_CONTEXTS, EDGE_INDEX, EDGE_TYPE)\n",
    "\n",
    "    loss = criterion(scores, CONTEXT_LABELS)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    R10 = Rat10(scores, CONTEXT_LABELS)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}, R@10={R10}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReProver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
